{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMr7BJqDxV3e0MiLmaW5eg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IanChoi2464/Machine-Learning-Colab/blob/main/9-3%20LSTM%20and%20GRU%20cell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "순환 신경망에서 빼놓을 수 없는 핵심 기술인 LSTM과 GRU 셀을 사용한 모델을 만들어 본다.\n",
        "# Keyword\n",
        "**LSTM**: Long Short-Term Memory, LSTM 셀은 타임스텝이 긴 데이터를 효과적으로 학습하기 위해 고안된 순환층, 입력 게이트, 삭제 게이트, 출력 게이트 역할을 하는 작은 셀이 포함되어 있음\\\n",
        "**셀 상태**: LSTM 셀은 은닉 상태 외에 셀 상태를 출력, 셀 상태는 다음 층으로 전달되지 않으며 현재 셀에서만 순환됨\\\n",
        "**GRU**: Gated Recurrent Unit, GRU 셀은 LSTM 셀의 간소화 버전으로 생각할 수 있지만 LSTM 셀에 못지않는 성능을 냄\n",
        "# Function\n",
        "### **Tensorflow**\n",
        "**LSTM**: LSTM 셀을 사용한 순환층 클래스\\\n",
        "첫 번째 매개변수에 뉴런의 개수를 지정\\\n",
        "dropout 매개변수에서 입력에 대한 드롭아웃 비율을 지정할 수 있음\\\n",
        "return_sequences 매개변수에서 모든 타임스텝의 은닉 상태를 출력할지 결정, 기본값은 False\\\n",
        "**GRU**: GRU 셀을 사용한 순환층 클래스\\\n",
        "첫 번째 매개변수에 뉴런의 개수를 지정\\\n",
        "dropout 매개변수에서 입력에 대한 드롭아웃 비율을 지정할 수 있음\\\n",
        "return_sequences 매개변수에서 모든 타임스텝의 은닉 상태를 출력할지 결정, 기본값은 False"
      ],
      "metadata": {
        "id": "uy5Rvv4N4ijK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO4j-LKI4aEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3bf523-b37d-4433-efea-a2f7cbf85814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\"\"\"IMDB 데이터 준비하기\"\"\"\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)\n",
        "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_seq = pad_sequences(train_input, maxlen=100)\n",
        "val_seq = pad_sequences(val_input, maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"LSTM1\"\"\"\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model.add(keras.layers.LSTM(8))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H60cg7DEG0D",
        "outputId": "41909888-0cb6-4682-ea06-310ad4363291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 16)           8000      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 8)                 800       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8809 (34.41 KB)\n",
            "Trainable params: 8809 (34.41 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"LSTM2. 훈련함수\"\"\"\n",
        "def model_fn(a_layer=None):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
        "  model.add(keras.layers.Dense(100, activation='relu'))\n",
        "  if a_layer:\n",
        "    model.add(a_layer)\n",
        "  model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def train_fn(model, name):\n",
        "  rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "  model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  checkpoint_cb = keras.callbacks.ModelCheckpoint(name, save_best_only=True)\n",
        "  early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "  history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "id": "nO6tOEdhH5L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"LSTM2\"\"\"\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "id": "CdyAYqitEG-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5213eecd-47f7-409f-ffe5-d40bde8c4ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 20s 52ms/step - loss: 0.6925 - accuracy: 0.5360 - val_loss: 0.6917 - val_accuracy: 0.5734\n",
            "Epoch 2/100\n",
            "  3/313 [..............................] - ETA: 12s - loss: 0.6911 - accuracy: 0.5781"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 16s 52ms/step - loss: 0.6904 - accuracy: 0.5935 - val_loss: 0.6889 - val_accuracy: 0.6112\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 27s 85ms/step - loss: 0.6861 - accuracy: 0.6227 - val_loss: 0.6827 - val_accuracy: 0.6380\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 28s 89ms/step - loss: 0.6753 - accuracy: 0.6552 - val_loss: 0.6646 - val_accuracy: 0.6750\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.6334 - accuracy: 0.7017 - val_loss: 0.5917 - val_accuracy: 0.7188\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 24s 75ms/step - loss: 0.5731 - accuracy: 0.7268 - val_loss: 0.5591 - val_accuracy: 0.7404\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 26s 83ms/step - loss: 0.5474 - accuracy: 0.7473 - val_loss: 0.5401 - val_accuracy: 0.7528\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 26s 83ms/step - loss: 0.5270 - accuracy: 0.7616 - val_loss: 0.5227 - val_accuracy: 0.7648\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 22s 70ms/step - loss: 0.5096 - accuracy: 0.7713 - val_loss: 0.5090 - val_accuracy: 0.7750\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 21s 67ms/step - loss: 0.4950 - accuracy: 0.7815 - val_loss: 0.5011 - val_accuracy: 0.7700\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 20s 65ms/step - loss: 0.4820 - accuracy: 0.7893 - val_loss: 0.4887 - val_accuracy: 0.7760\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 0.4711 - accuracy: 0.7913 - val_loss: 0.4785 - val_accuracy: 0.7824\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 21s 68ms/step - loss: 0.4615 - accuracy: 0.7963 - val_loss: 0.4706 - val_accuracy: 0.7890\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 22s 70ms/step - loss: 0.4533 - accuracy: 0.8022 - val_loss: 0.4640 - val_accuracy: 0.7922\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 0.4465 - accuracy: 0.8031 - val_loss: 0.4604 - val_accuracy: 0.7902\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 17s 54ms/step - loss: 0.4412 - accuracy: 0.8058 - val_loss: 0.4555 - val_accuracy: 0.7940\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 15s 49ms/step - loss: 0.4364 - accuracy: 0.8081 - val_loss: 0.4519 - val_accuracy: 0.7950\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 15s 48ms/step - loss: 0.4325 - accuracy: 0.8097 - val_loss: 0.4486 - val_accuracy: 0.7990\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 0.4292 - accuracy: 0.8120 - val_loss: 0.4466 - val_accuracy: 0.7994\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 16s 50ms/step - loss: 0.4269 - accuracy: 0.8123 - val_loss: 0.4442 - val_accuracy: 0.7976\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 16s 50ms/step - loss: 0.4245 - accuracy: 0.8122 - val_loss: 0.4424 - val_accuracy: 0.7994\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 21s 66ms/step - loss: 0.4227 - accuracy: 0.8134 - val_loss: 0.4423 - val_accuracy: 0.7958\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 0.4212 - accuracy: 0.8128 - val_loss: 0.4401 - val_accuracy: 0.7980\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 22s 70ms/step - loss: 0.4195 - accuracy: 0.8148 - val_loss: 0.4426 - val_accuracy: 0.7924\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 16s 50ms/step - loss: 0.4184 - accuracy: 0.8131 - val_loss: 0.4386 - val_accuracy: 0.7990\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4167 - accuracy: 0.8149 - val_loss: 0.4456 - val_accuracy: 0.7894\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 0.4160 - accuracy: 0.8124 - val_loss: 0.4387 - val_accuracy: 0.7988\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 19s 61ms/step - loss: 0.4148 - accuracy: 0.8152 - val_loss: 0.4360 - val_accuracy: 0.7992\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.4142 - accuracy: 0.8132 - val_loss: 0.4360 - val_accuracy: 0.7968\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 20s 64ms/step - loss: 0.4132 - accuracy: 0.8142 - val_loss: 0.4366 - val_accuracy: 0.7936\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4123 - accuracy: 0.8149 - val_loss: 0.4346 - val_accuracy: 0.7976\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 22s 70ms/step - loss: 0.4117 - accuracy: 0.8142 - val_loss: 0.4347 - val_accuracy: 0.8004\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 20s 64ms/step - loss: 0.4113 - accuracy: 0.8145 - val_loss: 0.4345 - val_accuracy: 0.7998\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 22s 70ms/step - loss: 0.4103 - accuracy: 0.8148 - val_loss: 0.4355 - val_accuracy: 0.8014\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4099 - accuracy: 0.8159 - val_loss: 0.4344 - val_accuracy: 0.7962\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 21s 68ms/step - loss: 0.4093 - accuracy: 0.8142 - val_loss: 0.4342 - val_accuracy: 0.8006\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4089 - accuracy: 0.8149 - val_loss: 0.4325 - val_accuracy: 0.7980\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - 15s 48ms/step - loss: 0.4085 - accuracy: 0.8155 - val_loss: 0.4342 - val_accuracy: 0.8022\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - 15s 48ms/step - loss: 0.4073 - accuracy: 0.8159 - val_loss: 0.4338 - val_accuracy: 0.8024\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - 15s 48ms/step - loss: 0.4074 - accuracy: 0.8156 - val_loss: 0.4329 - val_accuracy: 0.8020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"LSTM3\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IljV3Zv3EHJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"순환층에 드롭아웃\"\"\"\n",
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model2.add(keras.layers.LSTM(8, dropout=0.3))\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-dropout-model.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fF4ud5-7EHUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"2개의 층 연결하기1.\"\"\"\n",
        "model3 = keras.Sequential()\n",
        "model3.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))\n",
        "model3.add(keras.layers.LSTM(8, dropout=0.3))\n",
        "model3.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "XHIbqHYmEcoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"2개의 층 연결하기2.\"\"\"\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model3.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-2rnn-model.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "history = model3.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "id": "DiE5l6ecEhj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"2개의 층 연결하기3.\"\"\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "avIq4LMPElIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"GRU1\"\"\"\n",
        "model4 = keras.Sequential()\n",
        "model4.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model4.add(keras.layers.GRU(8))\n",
        "model4.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model4.summary()\n",
        "\n",
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model4.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-gru-model.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "history = model4.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-u0pyFY_EnBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"테스트\"\"\"\n",
        "test_seq = pad_sequences(test_input, maxlen=100)\n",
        "rnn_model = keras.models.load_model('best-2rnn-model.h5')\n",
        "rnn_model.evaluate(test_seq, test_target)"
      ],
      "metadata": {
        "id": "XlcTJhGYEtdg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}